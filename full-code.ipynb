{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Transformer : Performance Evaluation of Vanilla and Performer Transformers for Text-to-Cypher Query Generation","metadata":{}},{"cell_type":"markdown","source":"Projek ini merupakan projek kuliah deep learning yang membangun vanila transformer dan variant transformer untuk permasalahan cypher query generation.","metadata":{}},{"cell_type":"markdown","source":"## 1. Import and Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install wandb\n!pip install nltk\n!pip install torchinfo\nfrom torchinfo import summary\nimport nltk\nimport torch\nimport torch.nn as nn\nimport math\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom transformers import T5Tokenizer\nimport time\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.tokenize import word_tokenize\nnltk.download(\"punkt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:47:39.517111Z","iopub.execute_input":"2025-06-01T14:47:39.517530Z","iopub.status.idle":"2025-06-01T14:48:06.283500Z","shell.execute_reply.started":"2025-06-01T14:47:39.517495Z","shell.execute_reply":"2025-06-01T14:48:06.282892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Login Wandb","metadata":{}},{"cell_type":"code","source":"import wandb\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"a80fda938f801fd535d7f9884348f76b061048b0\"\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:48:06.285089Z","iopub.execute_input":"2025-06-01T14:48:06.285536Z","iopub.status.idle":"2025-06-01T14:48:16.124932Z","shell.execute_reply.started":"2025-06-01T14:48:06.285500Z","shell.execute_reply":"2025-06-01T14:48:16.124340Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.Building Transformer Variant (Performer)","metadata":{}},{"cell_type":"markdown","source":"### 1. Vanila Transformer","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module): # Pembuatan Class Positional Encoding Untuk Menambahkan  Positional pada embedding dengan menggunakan nn.Module pytorch\n    def __init__(self, emb_dim, max_len=64): # emb_dim merupakan dimensi dari embedding dan max_lens merupakan panjang maksimum sequences\n        super().__init__()\n        pe = torch.zeros(max_len, emb_dim) # Inisiasi tensor berukuran max_len x emb_dim yang berisikan zero\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Membuat tensor posisi yang memiliki nilai dari 0 - 1 yang digunakan untuk sinusoidal\n        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim)) # menghitung pembagi eksponensial untuk menentukan frekuensi sinus dan cosinus\n        pe[:, 0::2] = torch.sin(position * div_term) # menggunakan klom genap untuk nilai sinus \n        pe[:, 1::2] = torch.cos(position * div_term) # menggunakan kolom ganjil untuk cosinus\n        pe = pe.unsqueeze(0)  # menambahkan dimensi untuk batch\n        self.register_buffer('pe', pe) # memasukkan tensor pe sebagai buffer yang bukan trainable\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)] # melaukan positional encoding pe ke input x\n\n\ndef scaled_dot_product(q, k, v, mask=None): # Menghitung scale dot product untuk perhitungan attention\n    d_k = q.size(-1) #mengambil dimensi dari d_k\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k) # melakukan scaled dot antara q dan k.T\n    if mask is not None: # check jika di mask\n        scores = scores.masked_fill(mask == 0, float('-inf')) # jika di mask maka nilai posisi akan 0\n    attn = torch.softmax(scores, dim=-1) # Menerapkan softmax pada attention\n    return torch.matmul(attn, v), attn # menghasilkan output attention v x attention score\n\n\nclass MultiHeadAttention(nn.Module): # Pembuatan class multihead attention\n    def __init__(self, emb_dim, num_heads):\n        super().__init__()\n        assert emb_dim % num_heads == 0 # memastikan jika ukuran embedding dapat dibagi\n        self.d_k = emb_dim // num_heads # ukuran d_k merupakan dimensi per head\n        self.num_heads = num_heads\n\n        self.q_linear = nn.Linear(emb_dim, emb_dim) # Proyeksi q ke dimensi embedding \n        self.k_linear = nn.Linear(emb_dim, emb_dim)  # Proyeksi k ke dimensi embedding \n        self.v_linear = nn.Linear(emb_dim, emb_dim) # Proyeksi v ke dimensi embedding \n        self.out = nn.Linear(emb_dim, emb_dim) # menggabungkan semua head menjadi satu output\n\n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0) # menyimpan batch size untuk reshaping\n\n        def transform(x, linear):\n            x = linear(x) # Mengaplikasikan proyeksi linear\n            x = x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) # Mengubah shape menjadi (batch_size, num_heads, seq_len, d_k)\n            return x  \n\n        q, k, v = transform(q, self.q_linear), transform(k, self.k_linear), transform(v, self.v_linear) # Memproyeksikan masing-masing input untuk multihead processing\n        scores, attn = scaled_dot_product(q, k, v, mask) # menghitung attention score\n        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k) # Menggabungkan semua hasil dari head\n        return self.out(scores)\n\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, emb_dim, ff_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(emb_dim, ff_dim) # Memproyeksikan emb dim menjadi ff_dim\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(ff_dim, emb_dim) # Memproyeksikan ff_dim  menjadi emb_dim\n\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x))) # memproses linear1 terhadap x -> relu (linear1) -> linear2 (relu)\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(emb_dim, num_heads) # Penggunaan self attention layer\n        self.ff = PositionwiseFeedForward(emb_dim, ff_dim) # Feedforward network\n        self.norm1 = nn.LayerNorm(emb_dim) # Layernorm 1\n        self.norm2 = nn.LayerNorm(emb_dim) # Layernorm setlah feed forward\n        self.dropout = nn.Dropout(dropout) # Dropout untuk regularization\n\n    def forward(self, x, mask=None):\n        attn = self.self_attn(x, x, x, mask) # Menghitung self attention\n        x = self.norm1(x + self.dropout(attn)) # Residual + dripout + norm \n        ff_out = self.ff(x) # Residual + dropout + norm\n        x = self.norm2(x + self.dropout(ff_out)) # Output encoded token representation\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(emb_dim, num_heads) # Self-attention untuk decoder (masked)\n        self.cross_attn = MultiHeadAttention(emb_dim, num_heads) # Cross-attention ke output encoder\n        self.ff = PositionwiseFeedForward(emb_dim, ff_dim)  # Feedforward network\n        self.norm1 = nn.LayerNorm(emb_dim)  # LayerNorm setelah self-attention\n        self.norm2 = nn.LayerNorm(emb_dim) # LayerNorm setelah cross-attention\n        self.norm3 = nn.LayerNorm(emb_dim) # LayerNorm setelah feedforward\n        self.dropout = nn.Dropout(dropout) # Dropout untuk regularisasi\n\n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))  # Masked self-attention dengan residual dan layer norm\n        x = self.norm2(x + self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask))) # Cross-attention (dari encoder ke decoder) dengan residual dan layer norm\n        x = self.norm3(x + self.dropout(self.ff(x))) # Feedforward network dengan residual dan layer norm\n        return x\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim)              # Embedding untuk input token\n        self.pos_encoding = PositionalEncoding(emb_dim, max_len)        # Penambahan positional encoding\n        self.layers = nn.ModuleList([\n            EncoderLayer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)  # Beberapa encoder layer\n        ])\n\n    def forward(self, src, mask=None):\n        x = self.embedding(src)                                          # Konversi token ke vektor\n        x = self.pos_encoding(x)                                         # Menambahkan informasi posisi\n        for layer in self.layers:\n            x = layer(x, mask)                                          \n        return x                                                        \n\nclass TransformerDecoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim)              # Embedding untuk target token\n        self.pos_encoding = PositionalEncoding(emb_dim, max_len)        # Positional encoding untuk target\n        self.layers = nn.ModuleList([\n            DecoderLayer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)  # Beberapa decoder layer\n        ])\n        self.fc_out = nn.Linear(emb_dim, vocab_size)                    # Proyeksi ke ukuran vocab untuk prediksi\n\n    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n        x = self.embedding(tgt)                                         # Embed target token\n        x = self.pos_encoding(x)                                        # Tambahkan positional encoding\n        for layer in self.layers:\n            x = layer(x, enc_output, src_mask, tgt_mask)                # Lewatkan ke decoder layer\n        return self.fc_out(x)                                           # Output logits untuk prediksi token\n\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.encoder = TransformerEncoder(src_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n        self.decoder = TransformerDecoder(tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n\n    def make_subsequent_mask(self, size):\n        mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(1)\n        return mask  # Maskking\n\n    def forward(self, src, tgt, src_mask=None):\n        enc_output = self.encoder(src, src_mask)                        # Encode input source\n        tgt_mask = self.make_subsequent_mask(tgt.size(1)).to(tgt.device) # Mask untuk autoregressive decoding\n        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)     # Decode dengan cross-attention\n        return output                                                   # Output prediksi akhir\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Performer Transformer","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef elu_feature_map(x):\n    return F.elu(x) + 1  # Fungsi feature map untuk Performer: menggunakan ELU + 1 agar hasil tetap positif\n\n    \ndef linear_attention(q, k, v):\n    q = elu_feature_map(q)  # Terapkan feature map pada query\n    k = elu_feature_map(k)  # Terapkan feature map pada key\n\n    kv = torch.einsum('bhnd,bhne->bhde', k, v)  # Kalikan key dan value: bentuk tensor (batch, head, d_k, d_model)\n    z = 1 / (torch.einsum('bhnd,bhd->bhn', q, k.sum(dim=2)) + 1e-6)  # Normalisasi dengan penjumlahan key\n    out = torch.einsum('bhnd,bhde->bhne', q, kv)  # Hitung hasil attention (query * kv)\n    out = out * z.unsqueeze(-1)  # Terapkan normalisasi\n    return out  # Kembalikan output attention\n\nclass PerformerAttention(nn.Module):\n    def __init__(self, emb_dim, num_heads):\n        super().__init__()\n        assert emb_dim % num_heads == 0  # Pastikan emb_dim bisa dibagi rata ke semua head\n        self.d_k = emb_dim // num_heads  # Ukuran dimensi per head\n        self.num_heads = num_heads\n\n        # Linear projection untuk query, key, dan value\n        self.q_linear = nn.Linear(emb_dim, emb_dim)\n        self.k_linear = nn.Linear(emb_dim, emb_dim)\n        self.v_linear = nn.Linear(emb_dim, emb_dim)\n        self.out = nn.Linear(emb_dim, emb_dim)  # Proyeksi output akhir\n\n    def forward(self, q, k, v, mask=None):\n        bsz = q.size(0)  # Batch size\n\n        def transform(x, linear):\n            x = linear(x)  # Proyeksi linier\n            x = x.view(bsz, -1, self.num_heads, self.d_k).transpose(1, 2)  # Bentuk ulang ke (batch, head, seq_len, d_k)\n            return x\n\n        # Proyeksikan dan ubah bentuk query, key, value\n        q, k, v = transform(q, self.q_linear), transform(k, self.k_linear), transform(v, self.v_linear)\n\n        # Hitung linear attention\n        attn_output = linear_attention(q, k, v)\n\n        # Gabungkan kembali output dari semua head\n        out = attn_output.transpose(1, 2).contiguous().view(bsz, -1, self.num_heads * self.d_k)\n\n        return self.out(out)  # Proyeksi ke dimensi akhir model\n        return self.out(out)\n\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, emb_dim, ff_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(emb_dim, ff_dim)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(ff_dim, emb_dim)\n\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n\nclass EncoderLayerPerformer(nn.Module):\n    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        self.self_attn = PerformerAttention(emb_dim, num_heads)\n        self.ff = PositionwiseFeedForward(emb_dim, ff_dim)\n        self.norm1 = nn.LayerNorm(emb_dim)\n        self.norm2 = nn.LayerNorm(emb_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn))\n        ff_out = self.ff(x)\n        x = self.norm2(x + self.dropout(ff_out))\n        return x\n\nclass DecoderLayerPerformer(nn.Module):\n    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        self.self_attn = PerformerAttention(emb_dim, num_heads)\n        self.cross_attn = PerformerAttention(emb_dim, num_heads)\n        self.ff = PositionwiseFeedForward(emb_dim, ff_dim)\n        self.norm1 = nn.LayerNorm(emb_dim)\n        self.norm2 = nn.LayerNorm(emb_dim)\n        self.norm3 = nn.LayerNorm(emb_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n        x = self.norm2(x + self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask)))\n        x = self.norm3(x + self.dropout(self.ff(x)))\n        return x\n\nclass TransformerEncoderPerformer(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        self.pos_encoding = PositionalEncoding(emb_dim, max_len)\n        self.layers = nn.ModuleList([\n            EncoderLayerPerformer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)\n        ])\n\n    def forward(self, src, mask=None):\n        x = self.embedding(src)\n        x = self.pos_encoding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\nclass TransformerDecoderPerformer(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        self.pos_encoding = PositionalEncoding(emb_dim, max_len)\n        self.layers = nn.ModuleList([\n            DecoderLayerPerformer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)\n        ])\n        self.fc_out = nn.Linear(emb_dim, vocab_size)\n    \n    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n        x = self.embedding(tgt)\n        x = self.pos_encoding(x)\n        for layer in self.layers:\n            x = layer(x, enc_output, src_mask, tgt_mask)\n        return self.fc_out(x)\n\nclass TransformerPerformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.encoder = TransformerEncoderPerformer(src_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n        self.decoder = TransformerDecoderPerformer(tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n\n    def make_subsequent_mask(self, size):\n        mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(1)\n        return mask  \n\n    def forward(self, src, tgt, src_mask=None):\n        enc_output = self.encoder(src, src_mask)\n        tgt_mask = self.make_subsequent_mask(tgt.size(1)).to(tgt.device)\n        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:48:16.125680Z","iopub.execute_input":"2025-06-01T14:48:16.126182Z","iopub.status.idle":"2025-06-01T14:48:16.131704Z","shell.execute_reply.started":"2025-06-01T14:48:16.126155Z","shell.execute_reply":"2025-06-01T14:48:16.131007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Preparation","metadata":{}},{"cell_type":"markdown","source":"### 1. Tokenizer","metadata":{}},{"cell_type":"markdown","source":"Dataset neo4j/text2cypher-2025v1 digunakan untuk mengubah teks alami menjadi query Cypher. Tokenisasi dilakukan dengan T5Tokenizer dari model t5-base, yang mengubah teks menjadi token numerik. Token khusus seperti pad_token_id, eos_token_id, dan sos_token_id disiapkan untuk mengatur struktur input dan output sesuai kebutuhan model encoder-decoder.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"neo4j/text2cypher-2025v1\")\nfrom transformers import T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\npad_token_id = tokenizer.pad_token_id\nsos_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")  \neos_token_id = tokenizer.eos_token_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:48:16.298294Z","iopub.execute_input":"2025-06-01T14:48:16.298555Z","iopub.status.idle":"2025-06-01T14:48:24.495319Z","shell.execute_reply.started":"2025-06-01T14:48:16.298526Z","shell.execute_reply":"2025-06-01T14:48:24.494768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fungsi batch_preprocess digunakan untuk memproses data dari dataset text2cypher agar dapat digunakan dalam pelatihan model berbasis T5. Fungsi ini menangani dua skenario: mode batch dan mode non batch. Untuk setiap pasangan question, schema, dan cypher, teks input dibentuk dalam format terstruktur dan ditokenisasi dengan panjang maksimum 512 token. Tokenisasi dilakukan baik untuk input maupun label (Cypher query), dengan padding dan truncation otomatis. Fungsi juga menghitung panjang input dan label sebelum padding. Hasil akhirnya berupa dictionary berisi token yang telah diproses dan dapat langsung digunakan sebagai input model.","metadata":{}},{"cell_type":"code","source":"def batch_preprocess(example):\n    inputs = []\n\n    is_batch = isinstance(example[\"question\"], list)\n\n    if is_batch:\n        for schema, question, cypher in zip(example[\"schema\"], example[\"question\"], example[\"cypher\"]):\n            schema = schema or \"\"\n            input_text = (\n                f\"translate question to cypher:\\n\"\n                f\"<question> {question} </question>\\n\"\n                f\"<schema> {schema} </schema>\"\n            )\n\n            input_enc = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n            label_enc = tokenizer(cypher+tokenizer.eos_token, max_length=512, truncation=True, padding=\"max_length\")\n\n            input_enc[\"labels\"] = label_enc[\"input_ids\"]\n            input_enc[\"input_length\"] = len(tokenizer(input_text)[\"input_ids\"])\n            input_enc[\"label_length\"] = len(tokenizer(cypher)[\"input_ids\"])\n\n            inputs.append(input_enc)\n\n        if not inputs:\n            print(\"Seluruh batch difilter.\")\n            return {}\n\n        return {key: [d[key] for d in inputs] for key in inputs[0]}\n\n    # Single example mode\n    schema = example.get(\"schema\", \"\")\n    question = example[\"question\"]\n    cypher = example[\"cypher\"]\n\n    input_text = (\n        f\"translate question to cypher:\\n\"\n        f\"<question> {question} </question>\\n\"\n        f\"<schema> {schema} </schema>\"\n    )\n\n    input_enc = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n    label_enc = tokenizer(cypher, max_length=512, truncation=True, padding=\"max_length\")\n\n    input_enc[\"labels\"] = label_enc[\"input_ids\"]\n    input_enc[\"input_length\"] = len(tokenizer(input_text)[\"input_ids\"])\n    input_enc[\"label_length\"] = len(tokenizer(cypher)[\"input_ids\"])\n\n    return input_enc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:48:24.496190Z","iopub.execute_input":"2025-06-01T14:48:24.497088Z","iopub.status.idle":"2025-06-01T14:48:24.504086Z","shell.execute_reply.started":"2025-06-01T14:48:24.497065Z","shell.execute_reply":"2025-06-01T14:48:24.503375Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Baris kode tokenized_dataset = dataset.map(...) digunakan untuk melakukan praproses seluruh dataset menggunakan fungsi batch_preprocess. Proses ini dilakukan dalam bentuk batch dengan ukuran 32 dan dijalankan secara paralel menggunakan 4 proses (num_proc=4) untuk efisiensi waktu. Fungsi map akan menerapkan tokenisasi dan format input-output model ke setiap contoh dalam dataset, menghasilkan dataset baru yang telah siap untuk dilatih dalam pipeline model Transformer seperti T5.","metadata":{}},{"cell_type":"code","source":"tokenized_dataset = dataset.map(\n    batch_preprocess,\n    batched=True,\n    batch_size=32,\n    num_proc=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:48:24.504910Z","iopub.execute_input":"2025-06-01T14:48:24.505158Z","iopub.status.idle":"2025-06-01T14:49:57.915402Z","shell.execute_reply.started":"2025-06-01T14:48:24.505135Z","shell.execute_reply":"2025-06-01T14:49:57.914526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Dataloader","metadata":{}},{"cell_type":"markdown","source":"HuggingfaceCypherDataset adalah class warpper dataset dari Huggingface agar dapat digunakan oleh torch.utils.data.DataLoader.","metadata":{}},{"cell_type":"code","source":"class HuggingfaceCypherDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n        labels = torch.tensor(item[\"labels\"], dtype=torch.long)\n        return input_ids, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:49:57.916427Z","iopub.execute_input":"2025-06-01T14:49:57.916668Z","iopub.status.idle":"2025-06-01T14:49:57.922253Z","shell.execute_reply.started":"2025-06-01T14:49:57.916621Z","shell.execute_reply":"2025-06-01T14:49:57.921694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Kode ini mengatur proses pembentukan data loader untuk pelatihan dan pengujian menggunakan PyTorch. Dataset yang telah ditokenisasi terlebih dahulu diwrapper menggunakan HuggingfaceCypherDataset agar sesuai dengan DataLoader. Data pelatihan (train_data) dimuat dalam train_loader dengan opsi shuffle=True untuk memastikan data teracak setiap epoch, sedangkan data pengujian (test_data) dimuat dalam test_loader tanpa pengacakan. Ukuran batch ditentukan sebesar 32 sampel untuk setiap iterasi pelatihan dan evaluasi.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ntrain_data = HuggingfaceCypherDataset(tokenized_dataset[\"train\"])\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_data = HuggingfaceCypherDataset(tokenized_dataset[\"test\"])\ntest_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:49:57.923030Z","iopub.execute_input":"2025-06-01T14:49:57.923271Z","iopub.status.idle":"2025-06-01T14:49:58.031338Z","shell.execute_reply.started":"2025-06-01T14:49:57.923250Z","shell.execute_reply":"2025-06-01T14:49:58.030847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Training and Evaluation Utils","metadata":{}},{"cell_type":"markdown","source":"### 1. Training and evaluation","metadata":{}},{"cell_type":"markdown","source":"**evaluate(model, dataloader, criterion, device, write_samples=False)**\n\nFungsi ini digunakan untuk mengevaluasi performa model pada data validasi atau pengujian.\n\n* Model diset ke mode evaluasi (model.eval()), dan torch.no_grad() digunakan untuk menonaktifkan perhitungan gradien.\n\n* Untuk setiap batch, input (src) dan target (tgt) dipindahkan ke device (CPU/GPU).\n\n* tgt dipisah menjadi tgt_input (semua token kecuali terakhir) dan tgt_output (semua token kecuali pertama).\n\n* Model menghasilkan output berdasarkan src dan tgt_input, lalu loss dihitung.\n\n* Prediksi dievaluasi menggunakan argmax, dan akurasi dihitung dengan membandingkan prediksi dan target (tanpa memperhitungkan padding).\n\n* Nilai rata-rata loss dan akurasi keseluruhan dikembalikan.\n\n\n**train(model, dataloader, optimizer, criterion, device)**\n\n* Fungsi ini digunakan untuk melatih model selama satu epoch.\n\n* Model diset ke mode training (model.train()).\n\n* Untuk setiap batch, src dan tgt diproses serupa dengan evaluasi.\n\n* Optimizer di-reset (optimizer.zero_grad()), kemudian model melakukan forward pass.\n\n* Loss dihitung dan dilakukan backward pass (loss.backward()), lalu parameter diperbarui (optimizer.step()).\n\n* Akurasi dihitung berdasarkan token yang benar (mengabaikan padding).\n\n* Loop menggunakan tqdm untuk menampilkan progres batch, termasuk loss per batch.\n\n* Rata-rata loss dan akurasi epoch dikembalikan di akhir.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndef evaluate(model, dataloader, criterion, device,write_samples=False):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    correct_tokens = 0\n    total_bleu = 0\n    num_samples = 0\n    sample_logs = []\n    loop = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Validating\", leave=False)\n    with torch.no_grad():\n        for i, (src, tgt) in loop:\n            src, tgt = src.to(device), tgt.to(device)\n\n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n\n            output = model(src, tgt_input)\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n            total_loss += loss.item()\n\n            preds = output.argmax(dim=-1)\n            mask = (tgt_output != tokenizer.pad_token_id)\n            correct = (preds == tgt_output) & mask\n            correct_tokens += correct.sum().item()\n            total_tokens += mask.sum().item()\n\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n    return avg_loss, accuracy\n\n\n\ndef train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    correct_tokens = 0\n    total_tokens = 0\n\n    loop = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\", leave=False)\n\n    for i, (src, tgt) in loop:\n        src, tgt = src.to(device), tgt.to(device)\n\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        output = model(src, tgt_input)\n\n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        loss.backward()\n        optimizer.step()\n\n        preds = output.argmax(dim=-1)  # (batch, seq_len)\n        mask = (tgt_output != tokenizer.pad_token_id)\n        correct = (preds == tgt_output) & mask\n        correct_tokens += correct.sum().item()\n        total_tokens += mask.sum().item()\n\n        total_loss += loss.item()\n        loop.set_description(f\"Training [Batch {i+1}/{len(dataloader)}]\")\n        loop.set_postfix(batch_loss=loss.item())\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:49:58.032055Z","iopub.execute_input":"2025-06-01T14:49:58.032299Z","iopub.status.idle":"2025-06-01T14:49:58.051864Z","shell.execute_reply.started":"2025-06-01T14:49:58.032277Z","shell.execute_reply":"2025-06-01T14:49:58.051362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Train With Wandb","metadata":{}},{"cell_type":"markdown","source":"Fungsi train_with_wandb digunakan untuk melatih model Transformer dengan konfigurasi tertentu, melakukan pencatatan metrik ke platform Weights & Biases (WandB), dan menyimpan model terbaik berdasarkan nilai validation loss terendah. Proses pelatihan dilakukan dengan optimisasi Adam dan fungsi loss CrossEntropy, serta mendukung multiple GPU dengan DataParallel. Setiap epoch, fungsi ini mencatat metrik akurasi dan loss untuk data pelatihan dan validasi, serta menerapkan early stopping jika tidak ada peningkatan setelah sejumlah epoch tertentu. Model terbaik disimpan dan diunggah ke WandB sebagai artifact.","metadata":{}},{"cell_type":"code","source":"def train_with_wandb(config, run_name,transformer):\n    import wandb\n    wandb.init(project=\"cypher-transformer-generation-variant\", config=config, name=run_name,reinit=True)\n    patience = config.get(\"patience\", 3)\n    patience_counter = 0\n    # === Setup Model ===\n    model = transformer(\n        src_vocab_size=tokenizer.vocab_size,\n        tgt_vocab_size=tokenizer.vocab_size,\n        emb_dim=config['emb_dim'],\n        num_layers=config['num_layers'],\n        num_heads=config['num_heads'],\n        ff_dim=config['ff_dim'],\n        max_len=config['max_len']\n    )\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n    best_val_loss = float('inf')\n    best_model_path = f\"best_model_{run_name}.pt\"\n\n    for epoch in range(config['epochs']):\n        print(\">>> Starting training step...\", flush=True)\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n        print(\">>> Finished training step...\", flush=True)\n        print(\">>> Starting evaluation step...\", flush=True)\n        val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n\n        print(\">>> Finished evaluation step...\", flush=True)\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n        })\n\n        print(f\"[{run_name}] Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%} \")\n\n        # Save best model only\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), best_model_path)\n            wandb.run.summary[\"best_val_loss\"] = val_loss\n            wandb.run.summary[\"best_model_path\"] = best_model_path\n            artifact = wandb.Artifact(name=f\"{run_name}_best_model\", type=\"model\")\n            artifact.add_file(best_model_path)\n            wandb.log_artifact(artifact)\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    wandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:49:58.052568Z","iopub.execute_input":"2025-06-01T14:49:58.052851Z","iopub.status.idle":"2025-06-01T14:49:58.080114Z","shell.execute_reply.started":"2025-06-01T14:49:58.052828Z","shell.execute_reply":"2025-06-01T14:49:58.079617Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Model Configuration","metadata":{}},{"cell_type":"markdown","source":"6 Model konfigurasi digunakan untuk ditraining secara sequence dan dimonitoring menggunakan wand","metadata":{}},{"cell_type":"code","source":"model_configs = [\n        {\n        \"emb_dim\": 128,\n        \"num_layers\": 2,\n        \"num_heads\": 4,\n        \"ff_dim\": 256,\n        \"max_len\": 512,\n        \"lr\": 1e-4,\n        \"epochs\": 40,\n        \"model_name\":\"vanila_transformerV1\"\n        \"transformer\":Transformer,\n    },\n    {\n        \"emb_dim\": 128,\n        \"num_layers\": 4,\n        \"num_heads\": 4,\n        \"ff_dim\": 512,\n        \"max_len\": 512,\n        \"lr\": 1e-4,\n        \"epochs\": 40,\n        \"model_name\":\"vanila_transformerV2\",\n        \"transformer\":Transformer,\n    },\n    {\n        \"emb_dim\": 128,\n        \"num_layers\": 8,\n        \"num_heads\": 4,\n        \"ff_dim\": 256,\n        \"max_len\": 512,\n        \"lr\": 1e-4,\n        \"epochs\": 40,\n        \"model_name\":\"vanila_transformerV3\",\n        \"transformer\":Transformer,\n    },\n    {\n        \"emb_dim\": 128,\n        \"num_layers\": 2,\n        \"num_heads\": 4,\n        \"ff_dim\": 256,\n        \"max_len\": 512,\n        \"lr\": 1e-4,\n        \"epochs\": 40,\n        \"model_name\":\"variant_transformerV1\"\n        \"transformer\":TransformerPerformer,\n    },\n    {\n        \"emb_dim\": 128,\n        \"num_layers\": 4,\n        \"num_heads\": 4,\n        \"ff_dim\": 512,\n        \"max_len\": 512,\n        \"lr\": 1e-4,\n        \"epochs\": 40,\n        \"model_name\":\"variant_transformerV2\",\n        \"transformer\":TransformerPerformer,\n    },\n    {\n        \"emb_dim\": 128,\n        \"num_layers\": 8,\n        \"num_heads\": 4,\n        \"ff_dim\": 256,\n        \"max_len\": 512,\n        \"lr\": 1e-4,\n        \"epochs\": 40,\n        \"model_name\":\"variant_transformerV3\",\n        \"transformer\":TransformerPerformer,\n    }\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:49:58.080795Z","iopub.execute_input":"2025-06-01T14:49:58.080983Z","iopub.status.idle":"2025-06-01T14:49:58.104521Z","shell.execute_reply.started":"2025-06-01T14:49:58.080968Z","shell.execute_reply":"2025-06-01T14:49:58.104011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Train the model","metadata":{}},{"cell_type":"markdown","source":"Melakukan proses training untuk 6 kombinasi model","metadata":{}},{"cell_type":"code","source":"for model in model_configs:\n    config = model\n    run_name = model[\"model_name\"]\n    transformer = model[\"transformer\"]\n    train_with_wandb(config, run_name,transformer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:49:58.105087Z","iopub.execute_input":"2025-06-01T14:49:58.105240Z","execution_failed":"2025-06-01T14:50:32.504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Evaluasi Model","metadata":{}},{"cell_type":"markdown","source":"Evaluasi model dilakukan dengan mengambil artifact dari model yang telah ditraining dan disimpan pada wandb. Evaluasi yang dilakukan menggunakan loss, accuracy, dan BLEU","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"a80fda938f801fd535d7f9884348f76b061048b0\"\nimport wandb\nrun = wandb.init()\n\nmodels= {\n    \"vanila_v1\":\"fadhilelrizanda-student/cypher-transformer-generation-variant/vanila_transformerV1_best_model:v39\",\n    \"vanila_v2\":\"fadhilelrizanda-student/cypher-transformer-generation-variant/vanila_transformerV2_best_model:v13\",\n    \"vanila_v3\":\"fadhilelrizanda-student/cypher-transformer-generation-variant/vanila_transformerV3_best_model:v18\",\n    \"variant_v1\":\"fadhilelrizanda-student/cypher-transformer-generation-variant/variant_transformerV1_best_model:v41\",\n    \"variant_v2\":\"fadhilelrizanda-student/cypher-transformer-generation-variant/variant_transformerV2_best_model:v40\",\n    \"variant_v3\":\"fadhilelrizanda-student/cypher-transformer-generation-variant/variant_transformerV3_best_model:v12\"\n}\nmodel_artifacts = {}\n\nfor model in models:\n    artifact = run.use_artifact(models[model], type='model')\n    artifact_dir = artifact.download()\n    model_artifacts[model] = artifact_dir\n\n\n\nimport torch\nfrom collections import OrderedDict\n\n\nmodel_config = [\n    {\"name\":\"vanila_v1\",\n    \"path\":\"/kaggle/working/artifacts/vanila_transformerV1_best_model:v39/best_model_vanila_transformerV1.pt\",\n    \"config\":model_configs[0]},\n\n     {\"name\":\"vanila_v2\",\n    \"path\":\"/kaggle/working/artifacts/vanila_transformerV2_best_model:v13/best_model_vanila_transformerV2.pt\",\n    \"config\":model_configs[1]},\n\n     {\"name\":\"vanila_v3\",\n    \"path\":\"/kaggle/working/artifacts/vanila_transformerV3_best_model:v18/best_model_vanila_transformerV3.pt\",\n    \"config\":model_configs[2]},\n\n     {\"name\":\"variant_v1\",\n    \"path\":\"/kaggle/working/artifacts/variant_transformerV1_best_model:v41/best_model_variant_transformerV1.pt\",\n    \"config\":model_configs[0]},\n\n     {\"name\":\"variant_v2\",\n    \"path\":\"/kaggle/working/artifacts/variant_transformerV2_best_model:v40/best_model_variant_transformerV2.pt\",\n    \"config\":model_configs[1]},\n\n     {\"name\":\"variant_v3\",\n    \"path\":\"/kaggle/working/artifacts/variant_transformerV3_best_model:v12/best_model_variant_transformerV3.pt\",\n    \"config\":model_configs[2]},\n]\n\nfor i,model_info in enumerate(model_config):\n    config = model_info[\"config\"]\n    \n    if i>2:\n        model = TransformerPerformer(\n    src_vocab_size=tokenizer.vocab_size,\n    tgt_vocab_size=tokenizer.vocab_size,\n    emb_dim=config[\"emb_dim\"],\n    num_layers=config[\"num_layers\"],\n    num_heads=config[\"num_heads\"],\n    ff_dim=config[\"ff_dim\"],\n    max_len=config[\"max_len\"]\n)\n    else:\n        model = Transformer(\n    src_vocab_size=tokenizer.vocab_size,\n    tgt_vocab_size=tokenizer.vocab_size,\n    emb_dim=config[\"emb_dim\"],\n    num_layers=config[\"num_layers\"],\n    num_heads=config[\"num_heads\"],\n    ff_dim=config[\"ff_dim\"],\n    max_len=config[\"max_len\"]\n)\n\n    model_path = model_info[\"path\"]\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    raw_state_dict = torch.load(model_path, map_location=device)\n    \n    cleaned_state_dict = OrderedDict()\n    for k, v in raw_state_dict.items():\n        new_key = k.replace(\"module.\", \"\")  # remove 'module.' prefix\n        cleaned_state_dict[new_key] = v\n    \n    model.load_state_dict(cleaned_state_dict)\n    model.to(device)\n    model.eval()\n\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n    \n    \n    \n    avg_loss, accuracy, avg_bleu, avg_infer_time = evaluate(\n        model=model,\n        dataloader=test_loader,\n        criterion=criterion,\n        device=device\n    )\n    \n    print(f\"ðŸ“Š Evaluation Results for {model_info['name']}\")\n    print(f\"- Loss         : {avg_loss:.4f}\")\n    print(f\"- Accuracy     : {accuracy:.4f}\")\n    print(f\"- Avg BLEU     : {avg_bleu:.4f}\")\n    print(f\"- Inference Time (avg/batch): {avg_infer_time:.4f} seconds\\n\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Berdasarkan training yang telah dilakukan didapatkan hasil sebagai berikut\n\nðŸ“Š Evaluation Results for vanila_v1\n- Loss         : 0.1841\n- Accuracy     : 0.9460\n- Avg BLEU     : 0.1758\n- Inference Time (avg/batch): 0.0109 seconds\n\nðŸ“Š Evaluation Results for vanila_v2\n- Loss         : 0.3133\n- Accuracy     : 0.9145\n- Avg BLEU     : 0.1485\n- Inference Time (avg/batch): 0.0224 seconds\n\nðŸ“Š Evaluation Results for vanila_v3\n- Loss         : 0.3189\n- Accuracy     : 0.8947\n- Avg BLEU     : 0.1984\n- Inference Time (avg/batch): 0.0412 seconds\n\nðŸ“Š Evaluation Results for variant_v1\n- Loss         : 0.0635\n- Accuracy     : 0.9820\n- Avg BLEU     : 0.3242\n- Inference Time (avg/batch): 0.0080 seconds\n\nðŸ“Š Evaluation Results for variant_v2\n- Loss         : 2.4396\n- Accuracy     : 0.5385\n- Avg BLEU     : 0.0608\n- Inference Time (avg/batch): 0.0166 seconds\n\nðŸ“Š Evaluation Results for variant_v3\n- Loss         : 0.0089\n- Accuracy     : 0.9972\n- Avg BLEU     : 0.3854\n- Inference Time (avg/batch): 0.0297 seconds","metadata":{}}]}