{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:10.560713Z",
     "iopub.status.busy": "2025-05-26T07:25:10.560236Z",
     "iopub.status.idle": "2025-05-26T07:25:11.094276Z",
     "shell.execute_reply": "2025-05-26T07:25:11.093609Z",
     "shell.execute_reply.started": "2025-05-26T07:25:10.560683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:11.096648Z",
     "iopub.status.busy": "2025-05-26T07:25:11.096142Z",
     "iopub.status.idle": "2025-05-26T07:25:20.232271Z",
     "shell.execute_reply": "2025-05-26T07:25:20.231544Z",
     "shell.execute_reply.started": "2025-05-26T07:25:11.096627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:20.233212Z",
     "iopub.status.busy": "2025-05-26T07:25:20.232936Z",
     "iopub.status.idle": "2025-05-26T07:25:29.012020Z",
     "shell.execute_reply": "2025-05-26T07:25:29.011441Z",
     "shell.execute_reply.started": "2025-05-26T07:25:20.233194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"a80fda938f801fd535d7f9884348f76b061048b0\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:29.013189Z",
     "iopub.status.busy": "2025-05-26T07:25:29.012797Z",
     "iopub.status.idle": "2025-05-26T07:25:36.775077Z",
     "shell.execute_reply": "2025-05-26T07:25:36.774450Z",
     "shell.execute_reply.started": "2025-05-26T07:25:29.013171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer\n",
    "import time\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_dim, max_len=64):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, emb_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, emb_dim)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, emb_dim)\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.776220Z",
     "iopub.status.busy": "2025-05-26T07:25:36.775842Z",
     "iopub.status.idle": "2025-05-26T07:25:36.780844Z",
     "shell.execute_reply": "2025-05-26T07:25:36.780074Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.776202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn, v), attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.782138Z",
     "iopub.status.busy": "2025-05-26T07:25:36.781849Z",
     "iopub.status.idle": "2025-05-26T07:25:36.841248Z",
     "shell.execute_reply": "2025-05-26T07:25:36.840602Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.782112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert emb_dim % num_heads == 0\n",
    "        self.d_k = emb_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.k_linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.v_linear = nn.Linear(emb_dim, emb_dim)\n",
    "        self.out = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        def transform(x, linear):\n",
    "            x = linear(x)\n",
    "            x = x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "            return x  # (batch_size, heads, seq_len, d_k)\n",
    "\n",
    "        q, k, v = transform(q, self.q_linear), transform(k, self.k_linear), transform(v, self.v_linear)\n",
    "        scores, attn = scaled_dot_product(q, k, v, mask)\n",
    "        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        return self.out(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.843924Z",
     "iopub.status.busy": "2025-05-26T07:25:36.843733Z",
     "iopub.status.idle": "2025-05-26T07:25:36.853042Z",
     "shell.execute_reply": "2025-05-26T07:25:36.852469Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.843909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(emb_dim, ff_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(ff_dim, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.relu(self.linear1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.854131Z",
     "iopub.status.busy": "2025-05-26T07:25:36.853964Z",
     "iopub.status.idle": "2025-05-26T07:25:36.866514Z",
     "shell.execute_reply": "2025-05-26T07:25:36.865798Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.854117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ff = PositionwiseFeedForward(emb_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn))\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.867383Z",
     "iopub.status.busy": "2025-05-26T07:25:36.867219Z",
     "iopub.status.idle": "2025-05-26T07:25:36.881626Z",
     "shell.execute_reply": "2025-05-26T07:25:36.880948Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.867370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ff = PositionwiseFeedForward(emb_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "        self.norm3 = nn.LayerNorm(emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n",
    "        x = self.norm2(x + self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask)))\n",
    "        x = self.norm3(x + self.dropout(self.ff(x)))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.882567Z",
     "iopub.status.busy": "2025-05-26T07:25:36.882312Z",
     "iopub.status.idle": "2025-05-26T07:25:36.894145Z",
     "shell.execute_reply": "2025-05-26T07:25:36.893605Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.882534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_encoding = PositionalEncoding(emb_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        x = self.embedding(src)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.895198Z",
     "iopub.status.busy": "2025-05-26T07:25:36.894928Z",
     "iopub.status.idle": "2025-05-26T07:25:36.905462Z",
     "shell.execute_reply": "2025-05-26T07:25:36.904847Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.895177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.pos_encoding = PositionalEncoding(emb_dim, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(tgt)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return self.fc_out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.906206Z",
     "iopub.status.busy": "2025-05-26T07:25:36.906052Z",
     "iopub.status.idle": "2025-05-26T07:25:36.916486Z",
     "shell.execute_reply": "2025-05-26T07:25:36.915843Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.906194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(src_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n",
    "        self.decoder = TransformerDecoder(tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n",
    "\n",
    "    def make_subsequent_mask(self, size):\n",
    "        mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(1)\n",
    "        return mask  # (1, 1, tgt_len, tgt_len)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        tgt_mask = self.make_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:36.917574Z",
     "iopub.status.busy": "2025-05-26T07:25:36.917289Z",
     "iopub.status.idle": "2025-05-26T07:25:39.349578Z",
     "shell.execute_reply": "2025-05-26T07:25:39.348569Z",
     "shell.execute_reply.started": "2025-05-26T07:25:36.917535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "dataset = load_dataset(\"neo4j/text2cypher-2025v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:39.351065Z",
     "iopub.status.busy": "2025-05-26T07:25:39.350727Z",
     "iopub.status.idle": "2025-05-26T07:25:45.443472Z",
     "shell.execute_reply": "2025-05-26T07:25:45.442863Z",
     "shell.execute_reply.started": "2025-05-26T07:25:39.351033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "sos_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")  # We can define <pad> as start\n",
    "eos_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:45.444729Z",
     "iopub.status.busy": "2025-05-26T07:25:45.444179Z",
     "iopub.status.idle": "2025-05-26T07:25:45.449433Z",
     "shell.execute_reply": "2025-05-26T07:25:45.448601Z",
     "shell.execute_reply.started": "2025-05-26T07:25:45.444703Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    schema = example.get(\"schema\", \"\")\n",
    "    question = example[\"question\"]\n",
    "    cypher = example[\"cypher\"]\n",
    "\n",
    "    input_text = f\"translate question to cypher: <schema> {schema} </schema> {question}\"\n",
    "    \n",
    "    model_inputs = tokenizer(input_text, max_length=64, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(cypher, max_length=64, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:25:45.450951Z",
     "iopub.status.busy": "2025-05-26T07:25:45.450521Z",
     "iopub.status.idle": "2025-05-26T07:27:33.331935Z",
     "shell.execute_reply": "2025-05-26T07:27:33.331270Z",
     "shell.execute_reply.started": "2025-05-26T07:25:45.450924Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:27:33.333140Z",
     "iopub.status.busy": "2025-05-26T07:27:33.332784Z",
     "iopub.status.idle": "2025-05-26T07:27:33.338255Z",
     "shell.execute_reply": "2025-05-26T07:27:33.337485Z",
     "shell.execute_reply.started": "2025-05-26T07:27:33.333111Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HuggingfaceCypherDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.data = hf_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n",
    "        labels = torch.tensor(item[\"labels\"], dtype=torch.long)\n",
    "        return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:27:33.339269Z",
     "iopub.status.busy": "2025-05-26T07:27:33.339021Z",
     "iopub.status.idle": "2025-05-26T07:27:33.388708Z",
     "shell.execute_reply": "2025-05-26T07:27:33.387757Z",
     "shell.execute_reply.started": "2025-05-26T07:27:33.339253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = HuggingfaceCypherDataset(tokenized_dataset[\"train\"])\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = HuggingfaceCypherDataset(tokenized_dataset[\"test\"])\n",
    "test_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:27:33.389802Z",
     "iopub.status.busy": "2025-05-26T07:27:33.389446Z",
     "iopub.status.idle": "2025-05-26T07:27:33.404615Z",
     "shell.execute_reply": "2025-05-26T07:27:33.403132Z",
     "shell.execute_reply.started": "2025-05-26T07:27:33.389782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def compute_bleu(pred_str, ref_str):\n",
    "    ref_tokens = [word_tokenize(ref_str)]\n",
    "    pred_tokens = word_tokenize(pred_str)\n",
    "    return sentence_bleu(ref_tokens, pred_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device,write_samples=False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "    total_bleu = 0\n",
    "    num_samples = 0\n",
    "    sample_logs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # === Accuracy ===\n",
    "            preds = output.argmax(dim=-1)\n",
    "            mask = (tgt_output != tokenizer.pad_token_id)\n",
    "            correct = (preds == tgt_output) & mask\n",
    "            correct_tokens += correct.sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "            # === BLEU ===\n",
    "            for i in range(src.size(0)):\n",
    "                pred_ids = preds[i].tolist()\n",
    "                target_ids = tgt_output[i].tolist()\n",
    "\n",
    "                pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n",
    "\n",
    "                bleu = compute_bleu(pred_text, ref_text)\n",
    "                total_bleu += bleu\n",
    "                num_samples += 1\n",
    "    \n",
    "                # === Log up to 5 examples ===\n",
    "                if write_samples:\n",
    "                    if len(sample_logs) < 5:\n",
    "                        input_ids = src[i].tolist()\n",
    "                        input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        \n",
    "                        sample_logs.append({\n",
    "                            \"input\": input_text,\n",
    "                            \"ground_truth\": ref_text,\n",
    "                            \"prediction\": pred_text\n",
    "                        })\n",
    "    \n",
    "\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    avg_bleu = total_bleu / num_samples if num_samples > 0 else 0.0\n",
    "    return avg_loss, accuracy, avg_bleu,sample_logs\n",
    "\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_tokens = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    loop = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\", leave=False)\n",
    "\n",
    "    for i, (src, tgt) in loop:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input)\n",
    "\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # === Accuracy Computation ===\n",
    "        preds = output.argmax(dim=-1)  # (batch, seq_len)\n",
    "        mask = (tgt_output != tokenizer.pad_token_id)\n",
    "        correct = (preds == tgt_output) & mask\n",
    "        correct_tokens += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "\n",
    "        # === Loss and Logging ===\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Training [Batch {i+1}/{len(dataloader)}]\")\n",
    "        loop.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:27:33.405970Z",
     "iopub.status.busy": "2025-05-26T07:27:33.405726Z",
     "iopub.status.idle": "2025-05-26T07:27:33.424418Z",
     "shell.execute_reply": "2025-05-26T07:27:33.423710Z",
     "shell.execute_reply.started": "2025-05-26T07:27:33.405943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_with_wandb(config, run_name):\n",
    "    import wandb\n",
    "    wandb.init(project=\"cypher-transformer-generation\", config=config, name=run_name,id=run_name)\n",
    "    patience = config.get(\"patience\", 3)\n",
    "    patience_counter = 0\n",
    "    # === Setup Model ===\n",
    "    model = Transformer(\n",
    "        src_vocab_size=tokenizer.vocab_size,\n",
    "        tgt_vocab_size=tokenizer.vocab_size,\n",
    "        emb_dim=config['emb_dim'],\n",
    "        num_layers=config['num_layers'],\n",
    "        num_heads=config['num_heads'],\n",
    "        ff_dim=config['ff_dim'],\n",
    "        max_len=config['max_len']\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = f\"best_model_{run_name}.pt\"\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc,val_bleu,_ = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_bleu\": val_bleu\n",
    "        })\n",
    "\n",
    "        print(f\"[{run_name}] Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%} | BLEU: {val_bleu:.3f}\")\n",
    "\n",
    "        # Save best model only\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            wandb.run.summary[\"best_val_loss\"] = val_loss\n",
    "            wandb.run.summary[\"best_model_path\"] = best_model_path\n",
    "            artifact = wandb.Artifact(name=f\"{run_name}_best_model\", type=\"model\")\n",
    "            artifact.add_file(best_model_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss, val_acc, val_bleu, sample_logs = evaluate(model, test_loader, criterion, device,write_samples=True)\n",
    "    \n",
    "    if sample_logs:\n",
    "        table = wandb.Table(columns=[\"input\", \"ground_truth\", \"prediction\"])\n",
    "        for row in sample_logs:\n",
    "            table.add_data(row[\"input\"], row[\"ground_truth\"], row[\"prediction\"])\n",
    "        wandb.log({\n",
    "            \"final_val_loss\": val_loss,\n",
    "            \"final_val_acc\": val_acc,\n",
    "            \"final_val_bleu\": val_bleu,\n",
    "            \"example_predictions\": table\n",
    "        })\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:27:33.425351Z",
     "iopub.status.busy": "2025-05-26T07:27:33.425140Z",
     "iopub.status.idle": "2025-05-26T07:27:33.438196Z",
     "shell.execute_reply": "2025-05-26T07:27:33.437610Z",
     "shell.execute_reply.started": "2025-05-26T07:27:33.425336Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    {\n",
    "        \"emb_dim\": 128,\n",
    "        \"num_layers\": 2,\n",
    "        \"num_heads\": 4,\n",
    "        \"ff_dim\": 256,\n",
    "        \"max_len\": 64,\n",
    "        \"lr\": 1e-4,\n",
    "        \"epochs\": 40,\n",
    "        \"model_name\":\"vanila_transformerV1\"\n",
    "    },\n",
    "    {\n",
    "        \"emb_dim\": 128,\n",
    "        \"num_layers\": 4,\n",
    "        \"num_heads\": 4,\n",
    "        \"ff_dim\": 512,\n",
    "        \"max_len\": 64,\n",
    "        \"lr\": 1e-4,\n",
    "        \"epochs\": 40,\n",
    "        \"model_name\":\"vanila_transformerV2\"\n",
    "    },\n",
    "    {\n",
    "        \"emb_dim\": 128,\n",
    "        \"num_layers\": 8,\n",
    "        \"num_heads\": 8,\n",
    "        \"ff_dim\": 512,\n",
    "        \"max_len\": 64,\n",
    "        \"lr\": 1e-3,\n",
    "        \"epochs\": 40,\n",
    "        \"model_name\":\"vanila_transformerV3\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-26T07:27:33.441375Z",
     "iopub.status.busy": "2025-05-26T07:27:33.440904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i, config in enumerate(model_configs):\n",
    "    run_name = config[\"model_name\"]\n",
    "    train_with_wandb(config, run_name)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
