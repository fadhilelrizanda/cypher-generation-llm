{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:10.560236Z","iopub.execute_input":"2025-05-26T07:25:10.560713Z","iopub.status.idle":"2025-05-26T07:25:11.094276Z","shell.execute_reply.started":"2025-05-26T07:25:10.560683Z","shell.execute_reply":"2025-05-26T07:25:11.093609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install wandb\n!pip install nltk\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nnltk.download(\"punkt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:11.096142Z","iopub.execute_input":"2025-05-26T07:25:11.096648Z","iopub.status.idle":"2025-05-26T07:25:20.232271Z","shell.execute_reply.started":"2025-05-26T07:25:11.096627Z","shell.execute_reply":"2025-05-26T07:25:20.231544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"a80fda938f801fd535d7f9884348f76b061048b0\"\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:20.232936Z","iopub.execute_input":"2025-05-26T07:25:20.233212Z","iopub.status.idle":"2025-05-26T07:25:29.012020Z","shell.execute_reply.started":"2025-05-26T07:25:20.233194Z","shell.execute_reply":"2025-05-26T07:25:29.011441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom transformers import T5Tokenizer\nimport time\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_dim, max_len=64):\n        super().__init__()\n        pe = torch.zeros(max_len, emb_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * (-math.log(10000.0) / emb_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, emb_dim)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # x: (batch_size, seq_len, emb_dim)\n        return x + self.pe[:, :x.size(1)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:29.012797Z","iopub.execute_input":"2025-05-26T07:25:29.013189Z","iopub.status.idle":"2025-05-26T07:25:36.775077Z","shell.execute_reply.started":"2025-05-26T07:25:29.013171Z","shell.execute_reply":"2025-05-26T07:25:36.774450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size(-1)\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float('-inf'))\n    attn = torch.softmax(scores, dim=-1)\n    return torch.matmul(attn, v), attn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.775842Z","iopub.execute_input":"2025-05-26T07:25:36.776220Z","iopub.status.idle":"2025-05-26T07:25:36.780844Z","shell.execute_reply.started":"2025-05-26T07:25:36.776202Z","shell.execute_reply":"2025-05-26T07:25:36.780074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, emb_dim, num_heads):\n        super().__init__()\n        assert emb_dim % num_heads == 0\n        self.d_k = emb_dim // num_heads\n        self.num_heads = num_heads\n\n        self.q_linear = nn.Linear(emb_dim, emb_dim)\n        self.k_linear = nn.Linear(emb_dim, emb_dim)\n        self.v_linear = nn.Linear(emb_dim, emb_dim)\n        self.out = nn.Linear(emb_dim, emb_dim)\n\n    def forward(self, q, k, v, mask=None):\n        batch_size = q.size(0)\n\n        def transform(x, linear):\n            x = linear(x)\n            x = x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n            return x  # (batch_size, heads, seq_len, d_k)\n\n        q, k, v = transform(q, self.q_linear), transform(k, self.k_linear), transform(v, self.v_linear)\n        scores, attn = scaled_dot_product(q, k, v, mask)\n        scores = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n        return self.out(scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.781849Z","iopub.execute_input":"2025-05-26T07:25:36.782138Z","iopub.status.idle":"2025-05-26T07:25:36.841248Z","shell.execute_reply.started":"2025-05-26T07:25:36.782112Z","shell.execute_reply":"2025-05-26T07:25:36.840602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self, emb_dim, ff_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(emb_dim, ff_dim)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(ff_dim, emb_dim)\n\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.843733Z","iopub.execute_input":"2025-05-26T07:25:36.843924Z","iopub.status.idle":"2025-05-26T07:25:36.853042Z","shell.execute_reply.started":"2025-05-26T07:25:36.843909Z","shell.execute_reply":"2025-05-26T07:25:36.852469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(emb_dim, num_heads)\n        self.ff = PositionwiseFeedForward(emb_dim, ff_dim)\n        self.norm1 = nn.LayerNorm(emb_dim)\n        self.norm2 = nn.LayerNorm(emb_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        attn = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn))\n        ff_out = self.ff(x)\n        x = self.norm2(x + self.dropout(ff_out))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.853964Z","iopub.execute_input":"2025-05-26T07:25:36.854131Z","iopub.status.idle":"2025-05-26T07:25:36.866514Z","shell.execute_reply.started":"2025-05-26T07:25:36.854117Z","shell.execute_reply":"2025-05-26T07:25:36.865798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, emb_dim, num_heads, ff_dim, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(emb_dim, num_heads)\n        self.cross_attn = MultiHeadAttention(emb_dim, num_heads)\n        self.ff = PositionwiseFeedForward(emb_dim, ff_dim)\n        self.norm1 = nn.LayerNorm(emb_dim)\n        self.norm2 = nn.LayerNorm(emb_dim)\n        self.norm3 = nn.LayerNorm(emb_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        x = self.norm1(x + self.dropout(self.self_attn(x, x, x, tgt_mask)))\n        x = self.norm2(x + self.dropout(self.cross_attn(x, enc_output, enc_output, src_mask)))\n        x = self.norm3(x + self.dropout(self.ff(x)))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.867219Z","iopub.execute_input":"2025-05-26T07:25:36.867383Z","iopub.status.idle":"2025-05-26T07:25:36.881626Z","shell.execute_reply.started":"2025-05-26T07:25:36.867370Z","shell.execute_reply":"2025-05-26T07:25:36.880948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        self.pos_encoding = PositionalEncoding(emb_dim, max_len)\n        self.layers = nn.ModuleList([\n            EncoderLayer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)\n        ])\n\n    def forward(self, src, mask=None):\n        x = self.embedding(src)\n        x = self.pos_encoding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.882312Z","iopub.execute_input":"2025-05-26T07:25:36.882567Z","iopub.status.idle":"2025-05-26T07:25:36.894145Z","shell.execute_reply.started":"2025-05-26T07:25:36.882534Z","shell.execute_reply":"2025-05-26T07:25:36.893605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim)\n        self.pos_encoding = PositionalEncoding(emb_dim, max_len)\n        self.layers = nn.ModuleList([\n            DecoderLayer(emb_dim, num_heads, ff_dim) for _ in range(num_layers)\n        ])\n        self.fc_out = nn.Linear(emb_dim, vocab_size)\n\n    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n        x = self.embedding(tgt)\n        x = self.pos_encoding(x)\n        for layer in self.layers:\n            x = layer(x, enc_output, src_mask, tgt_mask)\n        return self.fc_out(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.894928Z","iopub.execute_input":"2025-05-26T07:25:36.895198Z","iopub.status.idle":"2025-05-26T07:25:36.905462Z","shell.execute_reply.started":"2025-05-26T07:25:36.895177Z","shell.execute_reply":"2025-05-26T07:25:36.904847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len):\n        super().__init__()\n        self.encoder = TransformerEncoder(src_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n        self.decoder = TransformerDecoder(tgt_vocab_size, emb_dim, num_layers, num_heads, ff_dim, max_len)\n\n    def make_subsequent_mask(self, size):\n        mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(1)\n        return mask  # (1, 1, tgt_len, tgt_len)\n\n    def forward(self, src, tgt, src_mask=None):\n        enc_output = self.encoder(src, src_mask)\n        tgt_mask = self.make_subsequent_mask(tgt.size(1)).to(tgt.device)\n        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.906052Z","iopub.execute_input":"2025-05-26T07:25:36.906206Z","iopub.status.idle":"2025-05-26T07:25:36.916486Z","shell.execute_reply.started":"2025-05-26T07:25:36.906194Z","shell.execute_reply":"2025-05-26T07:25:36.915843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login using e.g. `huggingface-cli login` to access this dataset\ndataset = load_dataset(\"neo4j/text2cypher-2025v1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:36.917289Z","iopub.execute_input":"2025-05-26T07:25:36.917574Z","iopub.status.idle":"2025-05-26T07:25:39.349578Z","shell.execute_reply.started":"2025-05-26T07:25:36.917535Z","shell.execute_reply":"2025-05-26T07:25:39.348569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\npad_token_id = tokenizer.pad_token_id\nsos_token_id = tokenizer.convert_tokens_to_ids(\"<pad>\")  # We can define <pad> as start\neos_token_id = tokenizer.eos_token_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:39.350727Z","iopub.execute_input":"2025-05-26T07:25:39.351065Z","iopub.status.idle":"2025-05-26T07:25:45.443472Z","shell.execute_reply.started":"2025-05-26T07:25:39.351033Z","shell.execute_reply":"2025-05-26T07:25:45.442863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(example):\n    schema = example.get(\"schema\", \"\")\n    question = example[\"question\"]\n    cypher = example[\"cypher\"]\n\n    input_text = f\"translate question to cypher: <schema> {schema} </schema> {question}\"\n    \n    model_inputs = tokenizer(input_text, max_length=64, truncation=True, padding=\"max_length\")\n    labels = tokenizer(cypher, max_length=64, truncation=True, padding=\"max_length\")\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:45.444179Z","iopub.execute_input":"2025-05-26T07:25:45.444729Z","iopub.status.idle":"2025-05-26T07:25:45.449433Z","shell.execute_reply.started":"2025-05-26T07:25:45.444703Z","shell.execute_reply":"2025-05-26T07:25:45.448601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset = dataset.map(preprocess, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:25:45.450521Z","iopub.execute_input":"2025-05-26T07:25:45.450951Z","iopub.status.idle":"2025-05-26T07:27:33.331935Z","shell.execute_reply.started":"2025-05-26T07:25:45.450924Z","shell.execute_reply":"2025-05-26T07:27:33.331270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass HuggingfaceCypherDataset(Dataset):\n    def __init__(self, hf_dataset):\n        self.data = hf_dataset\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        input_ids = torch.tensor(item[\"input_ids\"], dtype=torch.long)\n        labels = torch.tensor(item[\"labels\"], dtype=torch.long)\n        return input_ids, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:27:33.332784Z","iopub.execute_input":"2025-05-26T07:27:33.333140Z","iopub.status.idle":"2025-05-26T07:27:33.338255Z","shell.execute_reply.started":"2025-05-26T07:27:33.333111Z","shell.execute_reply":"2025-05-26T07:27:33.337485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nBATCH_SIZE = 32\n\ntrain_data = HuggingfaceCypherDataset(tokenized_dataset[\"train\"])\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_data = HuggingfaceCypherDataset(tokenized_dataset[\"test\"])\ntest_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:27:33.339021Z","iopub.execute_input":"2025-05-26T07:27:33.339269Z","iopub.status.idle":"2025-05-26T07:27:33.388708Z","shell.execute_reply.started":"2025-05-26T07:27:33.339253Z","shell.execute_reply":"2025-05-26T07:27:33.387757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndef compute_bleu(pred_str, ref_str):\n    ref_tokens = [word_tokenize(ref_str)]\n    pred_tokens = word_tokenize(pred_str)\n    return sentence_bleu(ref_tokens, pred_tokens, smoothing_function=SmoothingFunction().method1)\n\ndef evaluate(model, dataloader, criterion, device,write_samples=False):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n    correct_tokens = 0\n    total_bleu = 0\n    num_samples = 0\n    sample_logs = []\n\n    with torch.no_grad():\n        for src, tgt in dataloader:\n            src, tgt = src.to(device), tgt.to(device)\n\n            tgt_input = tgt[:, :-1]\n            tgt_output = tgt[:, 1:]\n\n            output = model(src, tgt_input)\n            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n            total_loss += loss.item()\n\n            # === Accuracy ===\n            preds = output.argmax(dim=-1)\n            mask = (tgt_output != tokenizer.pad_token_id)\n            correct = (preds == tgt_output) & mask\n            correct_tokens += correct.sum().item()\n            total_tokens += mask.sum().item()\n\n            # === BLEU ===\n            for i in range(src.size(0)):\n                pred_ids = preds[i].tolist()\n                target_ids = tgt_output[i].tolist()\n\n                pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True)\n                ref_text = tokenizer.decode(target_ids, skip_special_tokens=True)\n\n                bleu = compute_bleu(pred_text, ref_text)\n                total_bleu += bleu\n                num_samples += 1\n    \n                # === Log up to 5 examples ===\n                if write_samples:\n                    if len(sample_logs) < 5:\n                        input_ids = src[i].tolist()\n                        input_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n        \n                        sample_logs.append({\n                            \"input\": input_text,\n                            \"ground_truth\": ref_text,\n                            \"prediction\": pred_text\n                        })\n    \n\n    \n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n    avg_bleu = total_bleu / num_samples if num_samples > 0 else 0.0\n    return avg_loss, accuracy, avg_bleu,sample_logs\n\n\n\ndef train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    correct_tokens = 0\n    total_tokens = 0\n\n    loop = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\", leave=False)\n\n    for i, (src, tgt) in loop:\n        src, tgt = src.to(device), tgt.to(device)\n\n        tgt_input = tgt[:, :-1]\n        tgt_output = tgt[:, 1:]\n\n        optimizer.zero_grad()\n        output = model(src, tgt_input)\n\n        loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n        loss.backward()\n        optimizer.step()\n\n        # === Accuracy Computation ===\n        preds = output.argmax(dim=-1)  # (batch, seq_len)\n        mask = (tgt_output != tokenizer.pad_token_id)\n        correct = (preds == tgt_output) & mask\n        correct_tokens += correct.sum().item()\n        total_tokens += mask.sum().item()\n\n        # === Loss and Logging ===\n        total_loss += loss.item()\n        loop.set_description(f\"Training [Batch {i+1}/{len(dataloader)}]\")\n        loop.set_postfix(batch_loss=loss.item())\n\n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:27:33.389446Z","iopub.execute_input":"2025-05-26T07:27:33.389802Z","iopub.status.idle":"2025-05-26T07:27:33.404615Z","shell.execute_reply.started":"2025-05-26T07:27:33.389782Z","shell.execute_reply":"2025-05-26T07:27:33.403132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_with_wandb(config, run_name):\n    import wandb\n    wandb.init(project=\"cypher-transformer-generation\", config=config, name=run_name,id=run_name)\n    patience = config.get(\"patience\", 3)\n    patience_counter = 0\n    # === Setup Model ===\n    model = Transformer(\n        src_vocab_size=tokenizer.vocab_size,\n        tgt_vocab_size=tokenizer.vocab_size,\n        emb_dim=config['emb_dim'],\n        num_layers=config['num_layers'],\n        num_heads=config['num_heads'],\n        ff_dim=config['ff_dim'],\n        max_len=config['max_len']\n    )\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n    best_val_loss = float('inf')\n    best_model_path = f\"best_model_{run_name}.pt\"\n\n    for epoch in range(config['epochs']):\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n        val_loss, val_acc,val_bleu,_ = evaluate(model, test_loader, criterion, device)\n\n        wandb.log({\n            \"epoch\": epoch + 1,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc,\n            \"val_bleu\": val_bleu\n        })\n\n        print(f\"[{run_name}] Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Acc: {val_acc:.2%} | BLEU: {val_bleu:.3f}\")\n\n        # Save best model only\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), best_model_path)\n            wandb.run.summary[\"best_val_loss\"] = val_loss\n            wandb.run.summary[\"best_model_path\"] = best_model_path\n            artifact = wandb.Artifact(name=f\"{run_name}_best_model\", type=\"model\")\n            artifact.add_file(best_model_path)\n            wandb.log_artifact(artifact)\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    model.load_state_dict(torch.load(best_model_path))\n    model.eval()\n    \n    val_loss, val_acc, val_bleu, sample_logs = evaluate(model, test_loader, criterion, device,write_samples=True)\n    \n    if sample_logs:\n        table = wandb.Table(columns=[\"input\", \"ground_truth\", \"prediction\"])\n        for row in sample_logs:\n            table.add_data(row[\"input\"], row[\"ground_truth\"], row[\"prediction\"])\n        wandb.log({\n            \"final_val_loss\": val_loss,\n            \"final_val_acc\": val_acc,\n            \"final_val_bleu\": val_bleu,\n            \"example_predictions\": table\n        })\n    wandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:27:33.405726Z","iopub.execute_input":"2025-05-26T07:27:33.405970Z","iopub.status.idle":"2025-05-26T07:27:33.424418Z","shell.execute_reply.started":"2025-05-26T07:27:33.405943Z","shell.execute_reply":"2025-05-26T07:27:33.423710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_configs = [\n    {\n        \"emb_dim\": 128,\n        \"num_layers\": 2,\n        \"num_heads\": 4,\n        \"ff_dim\": 256,\n        \"max_len\": 64,\n        \"lr\": 1e-4,\n        \"epochs\": 50,\n        \"model_name\":\"vanila_transformerV1\"\n    },\n    {\n        \"emb_dim\": 128,\n        \"num_layers\": 4,\n        \"num_heads\": 4,\n        \"ff_dim\": 512,\n        \"max_len\": 64,\n        \"lr\": 1e-4,\n        \"epochs\": 50,\n        \"model_name\":\"vanila_transformerV2\"\n    },\n    {\n        \"emb_dim\": 64,\n        \"num_layers\": 8,\n        \"num_heads\": 8,\n        \"ff_dim\": 512,\n        \"max_len\": 64,\n        \"lr\": 1e-3,\n        \"epochs\": 50,\n        \"model_name\":\"vanila_transformerV3\"\n    }\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:27:33.425140Z","iopub.execute_input":"2025-05-26T07:27:33.425351Z","iopub.status.idle":"2025-05-26T07:27:33.438196Z","shell.execute_reply.started":"2025-05-26T07:27:33.425336Z","shell.execute_reply":"2025-05-26T07:27:33.437610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, config in enumerate(model_configs):\n    run_name = config[\"model_name\"]\n    train_with_wandb(config, run_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T07:27:33.440904Z","iopub.execute_input":"2025-05-26T07:27:33.441375Z"}},"outputs":[],"execution_count":null}]}